{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习基本概念及实践"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, argparse, sys, os, errno\n",
    "from functools import reduce\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import scipy\n",
    "import sklearn\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from bokeh.palettes import Category20c\n",
    "from ipywidgets import interact,interactive, FloatSlider,IntSlider, RadioButtons,Dropdown,Tab,Text\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.datasets import make_classification, make_regression, make_circles, make_moons, make_gaussian_quantiles\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, precision_score, \\\n",
    "    roc_curve, precision_recall_curve, average_precision_score, matthews_corrcoef, confusion_matrix\n",
    "from statsmodels.robust.scale import mad\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "random_state = np.random.RandomState(1289237)  #我固定numpy的随机种子，以使结果可重现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load plotting functions\n",
    "embed pdf; std_plot; display dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup figure template\n",
    "figure_template_path = 'bin'\n",
    "if figure_template_path not in sys.path:\n",
    "    sys.path.append(figure_template_path)\n",
    "from importlib import reload\n",
    "import figure_template\n",
    "#force reload of the module\n",
    "reload(figure_template)\n",
    "from figure_template import display_dataframe, embed_pdf_figure, embed_pdf_pages,std_plot,legendhandle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General intro of all most important concepts in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### 机器学习的定义与分类\n",
    "\n",
    "机器学习研究如何从数据中学习其隐藏的模式并预测未知数据的特征。\n",
    "\n",
    "根据预测变量是否已知，机器学习通常分为两类：**监督学习**和**无监督学习**。\n",
    " \n",
    "- 监督学习\n",
    "\n",
    "模型通过特征和类别标签作为构建模型的输入。如果目标变量（要预测的变量）是类别信息（例如正/负），该问题称为分类问题。如果目标变量是连续的（例如身高）则为回归问题。\n",
    "\n",
    "- 无监督学习\n",
    "\n",
    "目标变量是未指定的。模型的目的是确定内部数据的结构（cluster）。在模型拟合之后，我们可以将新来的样本分给cluster或生成与原始数据具有相似分布的样本。无监督学习也可以用于监督学习之前的数据预处理步骤。\n",
    "\n",
    "一个完整的、有效的机器学习工程项目却包括很多步骤，可以包括**数据导入，数据可视化理解，前处理，特征选择，模型训练，参数调整，模型预测，模型评估，后处理**等多个步骤，一个在真实世界中有效的模型可能需要工作者对数据的深入理解，以选择各个步骤合适的方法。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个最简单的数据集\n",
    "\n",
    "\n",
    "### 分类问题数据集\n",
    "我们可以产生一个标签为离散值的用于分类问题的数据集\n",
    "\n",
    "[sklearn.datasets.make_classification](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification) 可以从一个混合高斯分布中产生样本，并且可以控制样本数量，类别数量和特征数量。\n",
    "\n",
    "我们会产生一个数据集，共有1000个样本，两种类别，四种特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_classes=2, n_features=4,\n",
    "                           n_informative=2, n_redundant=0, n_clusters_per_class=1,\n",
    "                           class_sep=0.9, random_state=random_state)\n",
    "X.shape, y.shape #查看特征和标签的shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 类别和特征矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "for label in np.unique(y):\n",
    "    ax.scatter(X[y == label, 0], X[y == label, 1], s=10, label=str(label))\n",
    "ax.legend(title='Class')\n",
    "#embed_pdf_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA 可视化\n",
    "上面只用到了前两维的特征，这显然不够全面，回忆[PCA的内容](https://lulab.gitbook.io/training/part-iii.-case-studies/case-study-2.exseek/2.2.normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_markers = ('o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H', 'D', 'd', 'P', 'X')\n",
    "fontlegend = {'family':'Arial',\n",
    "                  'weight' : 'normal', \n",
    "                  'size' : 6.5*1}\n",
    "def PCA_plot_sns(ax,data,sampleclass,method = 'Origin'):\n",
    "    #X = log_transfrom(data).T\n",
    "    X = StandardScaler().fit_transform(data.T)\n",
    "    if method=='Origin':\n",
    "        X_pca=X\n",
    "    if method == 'PCA':\n",
    "        transform = PCA()\n",
    "        X_pca = transform.fit_transform(X)\n",
    "    elif method == 'tSNE':\n",
    "        transform = TSNE()\n",
    "        X_pca = transform.fit_transform(X)\n",
    "   \n",
    "    plot_table = pd.DataFrame(X_pca[:,:2])\n",
    "    plot_table.index = data.columns\n",
    "    plot_table = pd.concat((plot_table,sampleclass.loc[plot_table.index]),axis=1)\n",
    "    plot_table.columns = ['dimension_1','dimension_2','class']\n",
    "    classnum = np.unique(plot_table.iloc[:,2]).shape[0]\n",
    "\n",
    "    sns.scatterplot(ax=ax,data=plot_table,x=\"dimension_1\", y=\"dimension_2\",markers=filled_markers,alpha=0.5,\n",
    "                    palette=legendhandle(np.unique(plot_table['class'])), hue=\"class\",style=\"class\",s=30,linewidth=0.01)\n",
    "    \n",
    "    std_plot(ax,'Dimension 1','Dimension 2',\n",
    "             title=method, legendtitle='class',legendsort=False\n",
    "             ,xbins=6,ybins=6\n",
    "            )\n",
    "    legend = ax.legend(prop=fontlegend,\n",
    "     bbox_to_anchor=(1.2,0.9),framealpha=0,labelspacing=0.24)\n",
    "    ax.legend_.get_frame()._linewidth=0\n",
    "    fig.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整理成需要的输入格式，seaborn接受dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(X).head()),display(pd.DataFrame(y).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_table = pd.DataFrame(X)\n",
    "class_label = pd.DataFrame(y)\n",
    "class_label.columns = ['label']\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(12,5))\n",
    "PCA_plot_sns(ax[0], input_table.T,class_label,'Origin')\n",
    "PCA_plot_sns(ax[1], input_table.T,class_label,'PCA')\n",
    "#embed_pdf_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这说明，简单的线性分类可能难以奏效"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data scaling\n",
    "\n",
    "对于大多数机器学习算法，建议将feature scale到一个比较小的范围，以减少极端值的影响。\n",
    "\n",
    "feature的规模过大或者过小都会增加数值不稳定的风险并且还使损失函数更加难以优化。\n",
    "- 基于线性模型权重的特征选择方法会假定输入的feature在同样的规模上。\n",
    "- 基于梯度下降算法的模型（比如神经网络）的表现和收敛速度会被没有合理scale的数据显著影响。\n",
    "- 决策树和随机森林类算法对数据规模不太敏感，因为它们使用rule-based标准。\n",
    "\n",
    "常见的数据缩放方法包括：\n",
    ">\n",
    "- standard/z-score scaling\n",
    "\n",
    "Standard/z-score scaling first shift features to their centers(mean) and then divide by their standard deviation.\n",
    "This method is suitable for most continous features of approximately Gaussian distribution.\n",
    "\n",
    "$$ \\text{zscore}(x_{ij}^{'}) = \\frac{x_{ij} - \\mu _{ij}}{\\sigma _i} $$\n",
    "\n",
    "- min-max scaling\n",
    "\n",
    "Min-max scaling method scales data into range \\[0, 1\\].\n",
    "This method is suitable for data concentrated within a range and preserves zero values for sparse data.\n",
    "Min-max scaling is also sensitive to outliers in the data. Try removing outliers or clip data into\n",
    "a range before scaling.\n",
    "\n",
    "$$ \\text{min_max}(x_{ij}^{'}) = \\frac{x_{ij} - \\text{min}_k \\mathbf{x}_{ik}}\n",
    "{\\text{max}_k x_{ik} - \\text{min}_k x_{ik}} $$\n",
    "\n",
    "\n",
    "- abs-max scaling.\n",
    "\n",
    "Max-abs scaling method is similar to min-max scaling, but scales data into range \\[-1, 1\\].\n",
    "It does not shift/center the data and thus preserves signs (positive/negative) of features.\n",
    "Like min-max, max-abs is sensitive to outliers.\n",
    "\n",
    "$$ \\text{max_abs}(x_{ij}^{'}) = \\frac{x_{ij}}{\\text{max}_k \\vert x_{ik} \\vert} $$\n",
    "\n",
    "- robust scaling\n",
    "\n",
    "Robust scaling method use robust statistics (median, interquartile range) instead of mean and standard deviation.\n",
    "Median and IQR are less sensitive to outliers.\n",
    "For features with large numbers of outliers or largely deviates from normal distribution, \n",
    "robust scaling is recommended.\n",
    "\n",
    "$$ \\text{robust_scale}(x_{ij}^{'}) = \\frac{x_{ij} - \\text{median}_k x_{ik}}\n",
    "{Q_{0.75}(\\mathbf{x}_i) - Q_{0.25}(\\mathbf{x}_i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得注意的是，exSEEK采用的就是最后一种标准化的方法。另外注意区分机器学习中的常规操作：对特征的标准化和RNA-seq分析中常用的样本库大小归一化的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用standard/z-score scaling 对数据做scaling\n",
    "使用方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#产生模拟数据，1000个数据点，均值为10，标准差为2\n",
    "x = random_state.normal(10, 2, size=1000)\n",
    "fig, ax = plt.subplots(1,2,figsize=(16, 6))\n",
    "sns.distplot(np.ravel(x), ax=ax[0])\n",
    "sns.distplot(np.ravel(StandardScaler().fit_transform(x.reshape((-1, 1)))), ax=ax[1])\n",
    "ax[0].set_title('original data distribution',fontsize=20)\n",
    "ax[1].set_title('scaled data distribution by standard scaling',fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#产生模拟数据，1000个数据点，均值为10，标准差为2\n",
    "\n",
    "fig, ax = plt.subplots(2,2,figsize=(16, 16))\n",
    "x = random_state.normal(10, 2, size=1000)\n",
    "sns.distplot(np.ravel(StandardScaler().fit_transform(x.reshape((-1, 1)))), ax=ax[0,0])\n",
    "x = random_state.normal(10, 2, size=1000)\n",
    "sns.distplot(np.ravel(MinMaxScaler().fit_transform(x.reshape((-1, 1)))), ax=ax[0,1])\n",
    "x = random_state.normal(10, 2, size=1000)\n",
    "sns.distplot(np.ravel(MaxAbsScaler().fit_transform(x.reshape((-1, 1)))), ax=ax[1,0])\n",
    "x = random_state.normal(10, 2, size=1000)\n",
    "sns.distplot(np.ravel(RobustScaler().fit_transform(x.reshape((-1, 1)))), ax=ax[1,1])\n",
    "ax[0,0].set_title('scaled data distribution by standard scaling',fontsize=20)\n",
    "ax[0,1].set_title('scaled data distribution by min-max scaling',fontsize=20)\n",
    "ax[1,0].set_title('scaled data distribution by abs-max scaling',fontsize=20)\n",
    "ax[1,1].set_title('scaled data distribution by robust scaling',fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分数据得到训练集和测试集（training set & test set）\n",
    "\n",
    "到这里，我们已经对数据进行了一些分析，并且做了一些基本的预处理，接下来我们需要对数据进行划分，得到训练集和测试集，通过训练集中的数据训练模型，再通过测试集的数据评估模型的表现。\n",
    "\n",
    "因为模型总是会在某种程度上过拟合训练数据，因此在训练数据上评估模型是有偏的，模型在训练集上的表现总会比测试集上好一些。\n",
    "\n",
    "因为模型总是可以学到数据中隐藏的模式和分布，如果样本间彼此的差异比较大，过拟合问题就会得到一定程度的减轻。而如果数据的量比较大，模型在训练集和测试集上的表现差异就会减小。\n",
    "\n",
    "\n",
    "使用[train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "方法来随机的将80%的样本设置为训练样本， 将其余20%设置为测试样本。\n",
    "\n",
    "\n",
    "另一个常见的概念是验证集（validation set），通过将训练集再随机划分为训练集和验证集，进行多折交叉验证（[cross validation](https://www.zhihu.com/question/39259296)），可以帮助我们评估不同的模型，调整模型的超参数等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "print('number of training samples: {}, test samples: {}'.format(X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "在模型训练的过程中，模型内部的参数会调整以最小化[损失函数](http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/)。\n",
    "\n",
    "### 逻辑斯谛回归\n",
    "\n",
    "逻辑斯谛回归是一个简单但是非常有效的模型，与它的名字不同，逻辑斯谛回归用于解决分类问题，在二分类问题中被广泛使用。对于二分类问题，我们需要对每一个样本预测它属于哪一类（0或者1）。\n",
    "\n",
    "逻辑斯谛回归是一个线性分类模型，它会对输入的feature进行线性组合，然后将线性组合组合得到的值通过一个非线性的sigmoid函数映射为一个概率值(范围为0~1)。\n",
    "\n",
    "模型训练过程中，模型内部的参数（线性模型的权重）会调整使得模型的损失函数（真实label和预测label的交叉熵）最小。\n",
    "\n",
    "$$ p(y_i | \\mathbf{x}_i) = \\frac{1}{1 + \\text{exp} \\left( \\sum_{j=1}^M x_{ij} w_{j} + b \\right)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 调用逻辑回归模型并且训练模型\n",
    "使用sklearn封装好的模型进行模型的训练非常简单，以逻辑斯谛回归模型为例，只需要两行即可完成模型的训练。\n",
    "\n",
    "我们使用默认参数进行训练，核心的内容非常简单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型内部的参数\n",
    "对于逻辑斯谛回归模型来说，其内部的参数是线性模型的权重项和偏置项$w_{j} , b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_,model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 四个特征的各自的权重（贡献）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=pd.DataFrame(model.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来验证一下这个“聪明”的模型找到的**最大权重**的特征是不是真的是**最有用的**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2,figsize=(16, 8))\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        sns.distplot(X[y==0][:,i*2+j].ravel(), color='r',ax=ax[i,j])\n",
    "        sns.distplot(X[y==1][:,i*2+j].ravel(), color='b',ax=ax[i,j])\n",
    "        ax[i,j].set_title('classification by feature' +str(i*2+j+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看起来feature3确实是最利于分开两类样本的，给予feature3最大的权重，辅以其他三个feature的信息，就可以较好的区分开两类样本了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估\n",
    "\n",
    "### 在测试集上预测样本类别\n",
    "\n",
    "为了评估模型表现，我们需要对测试集样本进行预测，我们使用*predict*方法来预测样本类别，它会返回一个整数型array来表示不同的样本类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "最常用的评估分类模型表现的方法是构建一个[confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "\n",
    "Confusion matrix会总结模型正确和错误分类的样本数量，并将预测的样本分成如下四类： \n",
    "\n",
    "![Markdown](http://i1.fuimg.com/640680/fc92776744bf93b5.png)\n",
    "\n",
    "* **Accuracy \\(0 ~ 1\\)** \n",
    "\n",
    "summarizes both positive and negative predictions, but is biased if the classes are imbalanced:\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "* **Recall/sensitivity \\(0 ~ 1\\)**\n",
    "\n",
    "summarizes how well the model finds out positive samples:\n",
    "\n",
    "$$\\text{Recall/Sensitivity} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "* **Precision/positive predictive value \\(0 ~ 1\\)** \n",
    "\n",
    "summarizes how well the model finds out negative samples:\n",
    "\n",
    "$$\\text{Precision/Positive Predictive Value} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "* **F1 score \\(0 ~ 1\\)** \n",
    "\n",
    "balances between positive predictive value \\(PPV\\) and true positive rate \\(TPR\\) and is more suitable for imbalanced dataset:\n",
    "\n",
    "$$\\text{F1 score} = 2 \\frac{PPV \\cdot TPR}{PPV + TPR}$$\n",
    "\n",
    "* **Matthews correlation coefficient \\(MCC\\) \\(-1 ~ 1\\)** \n",
    "\n",
    "another metric that balances between recall and precision:\n",
    "\n",
    "$$\\text{MCC} = \\frac{TP \\times TN - FP \\times FN} {(TP + FN)(TP + FP)(TN + FP)(TN + FN)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建预测结果的Confusion matrix\n",
    "#使用scikit-learn的confusion_matrix方法即可得到模型预测结果的confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(y_test, y_pred), \n",
    "             columns=pd.Series(['Negative', 'Positive'], name='Predicted'),\n",
    "             index=pd.Series(['Negative', 'Positive'], name='True'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorers = {'accuracy': accuracy_score,\n",
    "           'recall': recall_score,\n",
    "           'precision': precision_score,\n",
    "           'f1': f1_score,\n",
    "           'mcc': matthews_corrcoef\n",
    "}\n",
    "for metric in scorers.keys():\n",
    "    print('{} = {}'.format(metric, scorers[metric](y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC曲线和Precision-Recall曲线\n",
    "\n",
    "有时，一个固定的cutoff不足以评估模型性能。\n",
    "Receiver Operating Characterisic（ROC）曲线和Precision-Recall曲线可以通过不同的cutoff评估模型的表现。 ROC曲线和Precision-Recall对于类别不平衡问题也有比较好的评估。与ROC曲线相比，recision-Recall曲线更适合类别极不平衡的数据集。\n",
    "\n",
    "ROC曲线下面积（AUROC）或average precision (AP)是一个单值，它总结了不同截止值下的模型平均表现，常常用于报告模型的分类表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 绘制ROC曲线和Precision-Recall曲线\n",
    "我们使用sklearn自带的*roc_curve*和*precision_recall_curve*方法来计算绘图需要的指标，这两个方法需要的输入为测试集每个样本的真实标签和模型预测的每个样本的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "# ROC curve\n",
    "y_score = model.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score[:, 1])\n",
    "ax = axes[0]\n",
    "ax.plot(fpr, tpr, label='AUROC = {:.4f}'.format(roc_auc_score(y_test, y_score[:, 1])))\n",
    "ax.plot([0, 1], [0, 1], linestyle='dashed')\n",
    "ax.set_xlabel('False positive rate')\n",
    "ax.set_ylabel('True positive rate')\n",
    "ax.set_title('ROC curve')\n",
    "ax.legend()\n",
    "# predision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_score[:, 1])\n",
    "ax = axes[1]\n",
    "ax.plot(precision, recall, label='AP = {:.4f}'.format(average_precision_score(y_test, y_score[:, 1])))\n",
    "ax.plot([0, 1], [1, 0], linestyle='dashed')\n",
    "ax.set_xlabel('Precision')\n",
    "ax.set_ylabel('Recall')\n",
    "ax.set_title('Precision-recall curve')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到AUROC和AP都接近于1，可以认为模型的分类效果很好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉验证\n",
    "\n",
    "交叉验证可以被用于在训练集中再随机划分出一部分验证集用于挑选模型的参数，也可以用于直接评估模型的表现。\n",
    "\n",
    "对于非常大的数据集，将数据集单独拆分为训练集和测试集就足够来评估模型性能。但是，对于小型数据集，测试样本仅代表一小部分未来预测中可能的样本，即对于小数据集，划分出的测试集可能因为样本数过少而不具有代表性。\n",
    "\n",
    "\n",
    "### K折（k-folds）交叉验证\n",
    "\n",
    "交叉验证是小型数据集模型评估的常用技术。\n",
    "在**k折交叉验证**中，数据集被均匀地划分为*k*个部分（folds）。\n",
    "在每轮验证中，模型在一个fold上进行测试，并在剩余的*（k-1）/ k *部分上进行训练。 \n",
    "\n",
    "K折交叉验证确保训练样本和测试样本之间没有重叠，K轮结束后，每个样本会被设置为测试样品一次。最后，模型平均表现是在* k*轮次中计算的。\n",
    "\n",
    "\n",
    "\n",
    "*scikit-learn*提供很多功能来[划分数据集]\n",
    "(http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection).\n",
    "\n",
    "这里我们使用[KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)\n",
    "来将数据集划分为10折，5和10是交叉验证中经常使用的折数。如果样本数量和计算资源允许，一般设置为10折。\n",
    "\n",
    "下面的代码展示*KFold*是如何划分数据集的，图片中每一行为一个轮次，每一行中黑色的box为该轮次的测试集\n",
    "\n",
    "注意我们是在训练集（80%样本）上进行交叉验证的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "\n",
    "kfold = KFold(n_splits=n_splits, random_state=random_state)\n",
    "is_train = np.zeros((n_splits, X_train.shape[0]), dtype=np.bool)\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    is_train[i, train_index] = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "ax.pcolormesh(is_train)\n",
    "ax.set_yticks(np.arange(n_splits) + 0.5)\n",
    "ax.set_yticklabels(np.arange(n_splits) + 1)\n",
    "ax.set_ylabel('Round')\n",
    "ax.set_xlabel('Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.zeros((n_splits, X_train.shape[0]), dtype=np.int32)\n",
    "predicted_scores = np.zeros((n_splits, X_train.shape[0]))\n",
    "\n",
    "for i in range(n_splits):\n",
    "    model.fit(X_train[is_train[i]], y_train[is_train[i]])\n",
    "    predictions[i] = model.predict(X_train)\n",
    "    predicted_scores[i] = model.predict_proba(X_train)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect evaluation metrics\n",
    "\n",
    "我们统计了模型10折交叉验证的指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_metrics = pd.DataFrame(np.zeros((n_splits*2, len(scorers) + 2)),\n",
    "                          columns=list(scorers.keys()) + ['roc_auc', 'average_precision'])\n",
    "cv_metrics.loc[:, 'dataset'] = np.empty(n_splits*2, dtype='U')\n",
    "for i in range(n_splits):\n",
    "    for metric in scorers.keys():\n",
    "        cv_metrics.loc[i*2 + 0, metric] = scorers[metric](y_train[is_train[i]], predictions[i, is_train[i]])\n",
    "        cv_metrics.loc[i*2 + 1, metric] = scorers[metric](y_train[~is_train[i]], predictions[i, ~is_train[i]])\n",
    "    cv_metrics.loc[i*2 + 0, 'roc_auc'] = roc_auc_score(y_train[is_train[i]], predicted_scores[i, is_train[i]])\n",
    "    cv_metrics.loc[i*2 + 1, 'roc_auc'] = roc_auc_score(y_train[~is_train[i]], predicted_scores[i, ~is_train[i]])\n",
    "    cv_metrics.loc[i*2 + 0, 'average_precision'] = average_precision_score(y_train[is_train[i]], \n",
    "                                                                           predicted_scores[i, is_train[i]])\n",
    "    cv_metrics.loc[i*2 + 1, 'average_precision'] = average_precision_score(y_train[~is_train[i]], \n",
    "                                                                           predicted_scores[i, ~is_train[i]])\n",
    "    cv_metrics.loc[i*2 + 0, 'dataset'] = 'train'\n",
    "    cv_metrics.loc[i*2 + 1, 'dataset'] = 'valid'\n",
    "\n",
    "cv_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结评估指标\n",
    "\n",
    "我们评估多轮交叉验证的平均指标如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_metrics_mean = cv_metrics.groupby('dataset').mean()\n",
    "cv_metrics_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们绘制散点图来展示模型七个指标：*accuracy,recall,precision,f1,mcc,roc_auc,average_precision*分别在十折交叉验证中的值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plot_data = pd.melt(cv_metrics, id_vars=['dataset'], var_name='metric', value_name='value')\n",
    "sns.stripplot(x='metric', y='value', hue='dataset', \n",
    "              dodge=True, jitter=True, data=plot_data, size=4, ax=ax)\n",
    "#sns.pointplot(x='metric', y='value', hue='dataset', data=plot_data, markers=\"d\", \n",
    "#              join=False, ci=None, ax=ax, dodge=True, palette='dark')\n",
    "ax.set_title('Model performance using 10-fold cross-validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC 和 PR 曲线\n",
    "\n",
    "对每一轮交叉验证结果，我们都可以绘制一条ROC/PR曲线，当我们把十条曲线绘制在一起时，更好的方法是绘制十轮结果的均值曲线，以及用阴影区域表示十轮结果的置信区间。\n",
    "> tips: Anaconda并没有集成seaborn的最新版本，如果运行代码时报错，请使用pip更新seaborn：`pip install seaborn==0.9.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interp\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "# ROC curve\n",
    "ax = axes[0]\n",
    "all_fprs = np.linspace(0, 1, 100)\n",
    "roc_curves = np.zeros((n_splits, len(all_fprs), 2))\n",
    "for i in range(n_splits):\n",
    "    fpr, tpr, thresholds = roc_curve(y_train[~is_train[i]], predicted_scores[i, ~is_train[i]])\n",
    "    roc_curves[i, :, 0] = all_fprs\n",
    "    roc_curves[i, :, 1] = interp(all_fprs, fpr, tpr)\n",
    "roc_curves = pd.DataFrame(roc_curves.reshape((-1, 2)), columns=['fpr', 'tpr'])\n",
    "sns.lineplot(x='fpr', y='tpr', data=roc_curves, ci='sd', ax=ax,\n",
    "             label='Test AUC = {:.4f}'.format(cv_metrics_mean.loc['valid', 'roc_auc']))\n",
    "#ax.plot(fpr, tpr, label='ROAUC = {:.4f}'.format(roc_auc_score(y_test, y_score[:, 1])))\n",
    "#ax.plot([0, 1], [0, 1], linestyle='dashed')\n",
    "ax.set_xlabel('False positive rate')\n",
    "ax.set_ylabel('True positive rate')\n",
    "ax.plot([0, 1], [0, 1], linestyle='dashed', color='gray')\n",
    "ax.set_title('ROC curve')\n",
    "ax.legend()\n",
    "\n",
    "# predision-recall curve\n",
    "ax = axes[1]\n",
    "all_precs = np.linspace(0, 1, 100)\n",
    "pr_curves = np.zeros((n_splits, len(all_precs), 2))\n",
    "for i in range(n_splits):\n",
    "    fpr, tpr, thresholds = precision_recall_curve(y_train[~is_train[i]], predicted_scores[i, ~is_train[i]])\n",
    "    pr_curves[i, :, 0] = all_precs\n",
    "    pr_curves[i, :, 1] = interp(all_precs, fpr, tpr)\n",
    "pr_curves = pd.DataFrame(pr_curves.reshape((-1, 2)), columns=['precision', 'recall'])\n",
    "sns.lineplot(x='precision', y='recall', data=pr_curves, ci='sd', ax=ax,\n",
    "             label='Test AP = {:.4f}'.format(cv_metrics_mean.loc['valid', 'average_precision']))\n",
    "\n",
    "ax.set_xlabel('Precision')\n",
    "ax.set_ylabel('Recall')\n",
    "ax.plot([0, 1], [1, 0], linestyle='dashed', color='gray')\n",
    "ax.set_title('Precision-recall curve')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实战 乳腺癌预测\n",
    "本部分会比上一部分更简单，更重要的是，你会很容易地发现简单的机器学习实战中的大量重复的代码的套路：重复的调用模型方式和绘图分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1= Malignant (Cancerous) - Present\n",
    "* 0= Benign (Not Cancerous) - Absent\n",
    "\n",
    "\n",
    "### 1.3 Get the Data\n",
    "The [Breast Cancer](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29) datasets is available machine learning repository maintained by the University of California, Irvine. The dataset contains **569 samples of malignant and benign tumor cells**. \n",
    "* The first two columns in the dataset store the unique ID numbers of the samples and the corresponding diagnosis (M=malignant, B=benign), respectively. \n",
    "* The columns 3-32 contain 30 real-value features that have been computed from digitized images of the cell nuclei, which can be used to build a model to predict whether a tumor is benign or malignant.\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "\ta) radius (mean of distances from center to points on the perimeter)\n",
    "\tb) texture (standard deviation of gray-scale values)\n",
    "\tc) perimeter\n",
    "\td) area\n",
    "\te) smoothness (local variation in radius lengths)\n",
    "\tf) compactness (perimeter^2 / area - 1.0)\n",
    "\tg) concavity (severity of concave portions of the contour)\n",
    "\th) concave points (number of concave portions of the contour)\n",
    "\ti) symmetry \n",
    "\tj) fractal dimension (\"coastline approximation\" - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "all_df = pd.read_csv('data/data.csv', index_col=False)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id column is redundant and not useful, we want to drop it\n",
    "all_df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Quick Glance on the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The info() method is useful to get a quick description of the data, in particular the total number of rows, \n",
    "# and each attribute’s type and number of non-null values\n",
    "all_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到除了诊断信息，其他信息都已经数值化了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the categorical attribute's distribution\n",
    "all_df['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=\"diagnosis\", data=all_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"_mean\" suffix features 直方图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = all_df.iloc[:, 1:11]\n",
    "data_mean.hist(bins=10, figsize=(15, 10), grid=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram the \"_se\" suffix features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = all_df.iloc[:, 11:21]\n",
    "data_mean.hist(bins=10, figsize=(15, 10), grid=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram the \"_worst\" suffix features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = all_df.iloc[:, 21:]\n",
    "data_mean.hist(bins=10, figsize=(15, 10), grid=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "**concavity** and **area** 可能服从exponential distribution ( ). \n",
    "\n",
    "**texture**, **smooth** and **symmetry** 可能服从Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density plots of the \"_mean\" suffix features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = all_df.iloc[:, 1:11]\n",
    "data_mean.plot(kind='density', subplots=True, layout=(4,3), sharex=False, sharey=False, fontsize=12, figsize=(15,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density plots of the \"_se\" suffix features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = all_df.iloc[:, 11:21]\n",
    "data_mean.plot(kind='density', subplots=True, layout=(4,3), sharex=False, sharey=False, fontsize=12, figsize=(15,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density plots of the \"_worst\" suffix features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.iloc[:, 21:]\n",
    "data_mean.plot(kind='density', subplots=True, layout=(4,3), sharex=False, sharey=False, fontsize=12, figsize=(15,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Visualize distribution of data via Box plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plots of the \"_mean\" suffix features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = all_df.iloc[:, 1:11]\n",
    "data_mean.plot(kind='box', subplots=True, layout=(4,4), sharex=False, sharey=False, fontsize=12, figsize=(15,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,figsize=(20,8))\n",
    "sns.boxplot(data=all_df.iloc[:, 1:11],ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15,10))\n",
    "fig.subplots_adjust(hspace =.2, wspace=.3)\n",
    "axes = axes.ravel()\n",
    "for i, col in enumerate(all_df.columns[1:11]):\n",
    "    _= sns.boxplot(y=col, x='diagnosis', data=all_df, ax=axes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plots of the \"_se\" suffix features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = all_df.iloc[:, 11:21]\n",
    "data_mean.plot(kind='box', subplots=True, layout=(4,4), sharex=False, sharey=False, fontsize=12, figsize=(15,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15,10))\n",
    "fig.subplots_adjust(hspace =.2, wspace=.3)\n",
    "axes = axes.ravel()\n",
    "for i, col in enumerate(all_df.columns[11:21]):\n",
    "    _= sns.boxplot(y=col, x='diagnosis', data=all_df, ax=axes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plots of the \"_worst\" suffix features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = all_df.iloc[:, 21:]\n",
    "data_mean.plot(kind='box', subplots=True, layout=(4,4), sharex=False, sharey=False, fontsize=12, figsize=(15,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15,10))\n",
    "fig.subplots_adjust(hspace =.2, wspace=.3)\n",
    "axes = axes.ravel()\n",
    "for i, col in enumerate(all_df.columns[21:]):\n",
    "    _= sns.boxplot(y=col, x='diagnosis', data=all_df, ax=axes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corrMatt = all_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corrMatt)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(20, 12))\n",
    "plt.title('Breast Cancer Feature Correlation')\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(260, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corrMatt, vmax=1.2, square=False, cmap=cmap, mask=mask, ax=ax, annot=True, fmt='.2g', linewidths=1);\n",
    "#sns.heatmap(corrMatt, mask=mask, vmax=1.2, square=True, annot=True, fmt='.2g', ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "可以发现 mean values 的相关性都比较大 between 1 - 0.75.\n",
    "* The mean area of the tissue nucleus has a strong positive correlation with mean values of radius and parameter.\n",
    "* Some paramters are moderately positive correlated (r between 0.5-0.75) are concavity and area, concavity and perimeter etc.\n",
    "* Likewise, we see some strong negative correlation between fractal_dimension with radius, texture, perimeter mean values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots of the \"_mean\" suffix features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(all_df[list(all_df.columns[1:11]) + ['diagnosis']], hue=\"diagnosis\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots of the \"_se\" suffix features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(all_df[list(all_df.columns[11:21]) + ['diagnosis']], hue=\"diagnosis\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots of the \"_worst\" suffix features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(all_df[list(all_df.columns[21:]) + ['diagnosis']], hue=\"diagnosis\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "* Mean values of cell radius, perimeter, area, compactness, concavity and concave points can be used in classification of the cancer. Larger values of these parameters tends to show a correlation with malignant tumors.\n",
    "* Mean values of texture, smoothness, symmetry or fractual dimension does not show a particular preference of one diagnosis over the other.\n",
    "* In any of the histograms there are no noticeable large outliers that warrants further cleanup.\n",
    "\n",
    "机器学习模型的好处：自动判断特征的权重，不需要提前的分析就可以直接输入模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Handling Categorical Attributes : Label encoding\n",
    "Here, we transform the class labels from their original string representation (M and B) into integers\n",
    "\n",
    "即二值化label\n",
    "\n",
    "如果是多余两类的label，使用one hot coding的方式编码即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "diagnosis_encoded = encoder.fit_transform(all_df['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恶性：1，良性：0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 标准化特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_df.drop('diagnosis', axis=1)\n",
    "y = all_df['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize the  data (center around 0 and scale to remove the variance).\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 PCA可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# dimensionality reduction\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "Xs_pca = pca.fit_transform(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_df = pd.DataFrame()\n",
    "PCA_df['PCA_1'] = Xs_pca[:,0]\n",
    "PCA_df['PCA_2'] = Xs_pca[:,1]\n",
    "PCA_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(PCA_df['PCA_1'][all_df['diagnosis'] == 'M'],PCA_df['PCA_2'][all_df['diagnosis'] == 'M'],'ro', alpha = 0.7, markeredgecolor = 'k')\n",
    "plt.plot(PCA_df['PCA_1'][all_df['diagnosis'] == 'B'],PCA_df['PCA_2'][all_df['diagnosis'] == 'B'],'bo', alpha = 0.7, markeredgecolor = 'k')\n",
    "\n",
    "plt.xlabel('PCA_1')\n",
    "plt.ylabel('PCA_2')\n",
    "plt.legend(['Malignant','Benign']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "Xs_tsne = tsne.fit_transform(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(Xs_tsne[y == 'M'][:, 0], Xs_tsne[y == 'M'][:, 1], 'ro', alpha = 0.7, markeredgecolor='k')\n",
    "plt.plot(Xs_tsne[y == 'B'][:, 0], Xs_tsne[y == 'B'][:, 1], 'bo', alpha = 0.7, markeredgecolor='k')\n",
    "plt.legend(['Malignant','Benign']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive model using Support Vector Machine (SVM)\n",
    "\n",
    "Support vector machines (SVMs) learning algorithm will be used to build the predictive model.  SVMs are one of the most popular classification algorithms, and have an elegant way of transforming nonlinear data so that one can use a linear algorithm to fit a linear model to the data (Cortes and Vapnik 1995)\n",
    "\n",
    "#### Important Parameters\n",
    "The important parameters in kernel SVMs are the\n",
    "* Regularization parameter C; \n",
    "* The choice of the kernel - (linear, radial basis function(RBF) or polynomial);\n",
    "* Kernel-specific parameters. \n",
    "\n",
    "gamma and C both control the complexity of the model, with large values in either resulting in a more complex model. Therefore, good settings for the two parameters are usually strongly correlated, and C and gamma should be adjusted together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# transform the class labels from their original string representation (M and B) into integers\n",
    "le = LabelEncoder()\n",
    "all_df['diagnosis'] = le.fit_transform(all_df['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_df.drop('diagnosis', axis=1) # drop labels for training set\n",
    "y = all_df['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stratified sampling. Divide records in training and testing sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the  data (center around 0 and scale to remove the variance).\n",
    "scaler = StandardScaler()\n",
    "Xs_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SVM classifier and train it on 70% of the data set.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', probability=True)\n",
    "clf.fit(Xs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_score = clf.score(Xs_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在测试集上的准确度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The classifier accuracy score is {:03.2f}'.format(classifier_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average of 3-fold cross-validation score using an SVC estimator.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "n_folds = 10\n",
    "clf_cv = SVC(C=1.0, kernel='rbf', degree=3, gamma='auto')\n",
    "cv_error = np.average(cross_val_score(clf_cv, Xs_train, y_train, cv=n_folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The {}-fold cross-validation accuracy score for this classifier is {:.2f}'.format(n_folds, cv_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Feature Selection & cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "使用相关性分析挑选3个特征，再做分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame('https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html',width=900,height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# model with just 3 features selected\n",
    "clf_fs_cv = Pipeline([\n",
    "    ('feature_selector', SelectKBest(f_classif, k=3)),\n",
    "    ('svc', SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', probability=True))\n",
    "])\n",
    "\n",
    "scores = cross_val_score(clf_fs_cv, Xs_train, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)\n",
    "avg = (100 * np.mean(scores), 100 * np.std(scores)/np.sqrt(scores.shape[0]))\n",
    "print(\"Average score and uncertainty: (%.2f +- %.3f)%%\"  %avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看起来，只需要三个特征就已经足够取得很好的结果了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC and metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The confusion matrix helps visualize the performance of the algorithm.\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred = clf.fit(Xs_train, y_train).predict(Xs_test)\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengthy way to plot confusion matrix, a shorter way using seaborn is also shown somewhere downa\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "     for j in range(cm.shape[1]):\n",
    "         ax.text(x=j, y=i,\n",
    "                s=cm[i, j], \n",
    "                va='center', ha='center')\n",
    "classes=[\"Benign\",\"Malignant\"]\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "plt.xlabel('Predicted Values', )\n",
    "plt.ylabel('Actual Values');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "# Plot the receiver operating characteristic curve (ROC).\n",
    "plt.figure(figsize=(10,8))\n",
    "probas_ = clf.predict_proba(Xs_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC fold (area = %0.2f)' % (roc_auc))\n",
    "plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate | 1 - specificity (1 - Benign recall)')\n",
    "plt.ylabel('True Positive Rate | Sensitivity (Malignant recall)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.axes().set_aspect(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the SVM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Importance of optimizing a classifier\n",
    "\n",
    "可以调整其中的两个超参数\n",
    "* the value of C (how much to relax the margin) \n",
    "* and the type of kernel. \n",
    "\n",
    "The default for SVM (the SVC class) is to use the Radial Basis Function (RBF) kernel with a C value set to 1.0. We will perform a grid search using 5-fold cross validation with a standardized copy of the training dataset. We will try a number of simpler kernel types and C values with less bias and more bias (less than and more than 1.0 respectively).\n",
    "\n",
    "Python scikit-learn provides two simple methods for algorithm parameter tuning:\n",
    "* Grid Search Parameter Tuning. \n",
    "* Random Search Parameter Tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Train classifiers.\n",
    "kernel_values = [ 'linear', 'poly', 'rbf', 'sigmoid' ]\n",
    "param_grid = {'C': np.logspace(-3, 2, 6), 'gamma': np.logspace(-3, 2, 6), 'kernel': kernel_values}\n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\n",
    "grid.fit(Xs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = grid.best_estimator_\n",
    "best_clf.probability = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_clf.fit(Xs_train, y_train).predict(Xs_test)\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using seaborn to plot confusion matrix\n",
    "classes=[\"Benign\",\"Malignant\"]\n",
    "df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "fig = plt.figure(figsize=(3,3))\n",
    "ax = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "ax.yaxis.set_ticklabels(ax.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "ax.xaxis.set_ticklabels(ax.xaxis.get_ticklabels(), rotation=45, ha='right')\n",
    "plt.xlabel('Predicted Values', )\n",
    "plt.ylabel('Actual Values');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualizing the SVM Boundary\n",
    "\n",
    "Based on the best classifier that we got from our optimization process we would now try to visualize the decision boundary of the SVM. In order to visualize the SVM decision boundary we need to reduce the multi-dimensional data to two dimension. We will resort to applying the linear PCA transformation that will transofrm our data to a lower dimensional subspace (from 30D to 2D in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA by fitting the scaled data with only two dimensions\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Transform the original data using the PCA fit above\n",
    "Xs_train_pca = pca.fit_transform(Xs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first two PCA features. We could avoid this by using a two-dim dataset\n",
    "X = Xs_train_pca\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mesh of values from the 1st two PCA components\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=10.0, kernel='rbf', gamma=0.001)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.4)\n",
    "plt.plot(Xs_train_pca[y_train == 1][:, 0],Xs_train_pca[y_train == 1][:, 1], 'ro', alpha=0.8, markeredgecolor='k', label='Malignant')\n",
    "plt.plot(Xs_train_pca[y_train == 0][:, 0],Xs_train_pca[y_train == 0][:, 1], 'bo', alpha=0.8, markeredgecolor='k', label='Benign')\n",
    "\n",
    "svs = clf.support_vectors_\n",
    "plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#00AD00', edgecolors='k', label='Support Vectors')\n",
    "    \n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xlabel('PCA_1')\n",
    "plt.ylabel('PCA_2')\n",
    "plt.title('Decision Boundary of SVC with RBF kernel')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6. 自动化整个流程\n",
    "### 6.1 数据准备和建模流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Evaluate Some Algorithms\n",
    "Now it is time to create some models of the data and estimate their accuracy on unseen data. Here is what we are going to cover in this step:\n",
    "1. Separate out a validation dataset.\n",
    "2. Setup the test harness to use 10-fold cross validation.\n",
    "3. Build 5 different models  \n",
    "4. Select the best model\n",
    "\n",
    "#### Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "all_df = pd.read_csv('data/data.csv', index_col=False)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id column is redundant and not useful, we want to drop it\n",
    "all_df.drop('id', axis =1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# transform the class labels from their original string representation (M and B) into integers\n",
    "le = LabelEncoder()\n",
    "all_df['diagnosis'] = le.fit_transform(all_df['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_df.drop('diagnosis', axis=1) # drop labels for training set\n",
    "y = all_df['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide records in training and testing sets: stratified sampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the  data (center around 0 and scale to remove the variance).\n",
    "scaler = StandardScaler()\n",
    "Xs_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Evaluate Algorithms: Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "num_instances = len(X_train)\n",
    "scoring = 'accuracy'\n",
    "results = []\n",
    "names = []\n",
    "for name, model in tqdm(models):\n",
    "    kf = KFold(n_splits=num_folds, random_state=random_state)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kf, scoring=scoring, n_jobs=-1)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('10-Fold cross-validation accuracy score for the training data for all the classifiers') \n",
    "for name, cv_results in zip(names, results):\n",
    "    print(\"%-10s: %.6f (%.6f)\" % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "选择尽量简洁同时准确率高的模型是最好的，同时我们可以看一下模型在十折上的平均表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "plt.title( 'Algorithm Comparison' )\n",
    "plt.boxplot(results)\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('10 Fold CV Scores')\n",
    "plt.xticks(np.arange(len(names)) + 1, names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "为什么SVM的结果会这么差？因为SVM的数据的分布很敏感，需要将特征做标准化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Evaluate Algorithms: Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LogisticRegression())])))\n",
    "pipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()),('LDA', LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeClassifier())])))\n",
    "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB', GaussianNB())])))\n",
    "pipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC())])))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kf = KFold(n_splits=num_folds, random_state=random_state)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kf, scoring=scoring, n_jobs=-1)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('10-Fold cross-validation accuracy score for the training data for all the classifiers') \n",
    "for name, cv_results in zip(names, results):\n",
    "    print(\"%-10s: %.6f (%.6f)\" % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "plt.title( 'Algorithm Comparison' )\n",
    "plt.boxplot(results)\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('10 Fold CV Scores')\n",
    "plt.xticks(np.arange(len(names)) + 1, names, rotation=\"90\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "SVM的结果现在非常好了，同时，深入挖掘几个表现较好的模型，调整其参数，我们也许可以将结果优化的更好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Algorithm Tuning\n",
    "In this section we investigate tuning the parameters for three algorithms that show promise from the spot-checking in the previous section: LR, LDA and SVM.\n",
    "\n",
    "#### Tuning hyper-parameters - SVC estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Support Vector Classifier Pipeline\n",
    "pipe_svc = Pipeline([('scl', StandardScaler()),\n",
    "                     ('pca', PCA(n_components=2)),\n",
    "                     ('clf', SVC(probability=True, verbose=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Pipeline to training data and score\n",
    "scores = cross_val_score(estimator=pipe_svc, X=X_train, y=y_train, cv=10, n_jobs=-1, verbose=0)\n",
    "print('SVC Model Training Accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Tune Hyperparameters\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_grid = [{'clf__C': param_range,'clf__kernel': ['linear']},\n",
    "              {'clf__C': param_range,'clf__gamma': param_range, 'clf__kernel': ['rbf']}]\n",
    "\n",
    "gs_svc = GridSearchCV(estimator=pipe_svc,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=10,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs_svc = gs_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_svc.best_estimator_.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "gs_svc.best_estimator_.named_steps['clf'].coef_\n",
    "gs_svc.best_estimator_.named_steps['clf'].support_vectors_\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVC Model Tuned Parameters Best Score: ', gs_svc.best_score_)\n",
    "print('SVC Model Best Parameters: ', gs_svc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the hyper-parameters - k-NN hyperparameters\n",
    " For our standard k-NN implementation, there are two primary hyperparameters that we’ll want to tune:\n",
    "\n",
    "* The number of neighbors k.\n",
    "* The distance metric/similarity function.\n",
    "\n",
    "Both of these values can dramatically affect the accuracy of our k-NN classifier. Grid object is ready to do 10-fold cross validation on a KNN model using classification accuracy as the evaluation metric. In addition, there is a parameter grid to repeat the 10-fold cross validation process 30 times. Each time, the n_neighbors parameter should be given a different value from the list.\n",
    "\n",
    "We can't give `GridSearchCV` just a list  \n",
    "We've to specify `n_neighbors` should take on 1 through 30  \n",
    "We can set `n_jobs` = -1 to run computations in parallel (if supported by your computer and OS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "\n",
    "pipe_knn = Pipeline([('scl', StandardScaler()),\n",
    "                     ('pca', PCA(n_components=2)),\n",
    "                     ('clf', KNeighborsClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Pipeline to training data and score\n",
    "scores = cross_val_score(estimator=pipe_knn, \n",
    "                         X=X_train, \n",
    "                         y=y_train, \n",
    "                         cv=10,\n",
    "                         n_jobs=-1)\n",
    "print('Knn Model Training Accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Hyperparameters\n",
    "param_range = range(1, 31)\n",
    "param_grid = [{'clf__n_neighbors': param_range}]\n",
    "# instantiate the grid\n",
    "grid = GridSearchCV(estimator=pipe_knn, \n",
    "                    param_grid=param_grid, \n",
    "                    cv=10, \n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=-1)\n",
    "gs_knn = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Knn Model Tuned Parameters Best Score: ', gs_knn.best_score_)\n",
    "print('Knn Model Best Parameters: ', gs_knn.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 SVC最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best parameters\n",
    "final_clf_svc = gs_svc.best_estimator_\n",
    "\n",
    "# Get Final Scores\n",
    "scores = cross_val_score(estimator=final_clf_svc,\n",
    "                         X=X_train,\n",
    "                         y=y_train,\n",
    "                         cv=10,\n",
    "                         n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final Model Training Accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))\n",
    "print('Final Accuracy on Test set: %.5f' % final_clf_svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf_svc.fit(X_train, y_train)\n",
    "y_pred = final_clf_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# references\n",
    "- https://mp.weixin.qq.com/s/o3Gkey3SnYBcZGdOWojDXQ\n",
    "- https://github.com/rasbt/python-machine-learning-book\n",
    "- https://github.com/susanli2016/Machine-Learning-with-Python/\n",
    "- https://github.com/apachecn/AiLearning"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284.391px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
